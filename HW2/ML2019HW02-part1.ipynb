{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 2: Part 1 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem just right after the words **BEGIN SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "* The are problems with \\* mark - they are not obligatory. You can get **EXTRA POINTS** for solving them.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your theoretical derivations within such blocks:\n",
    "```markdown\n",
    "**BEGIN Solution**\n",
    "\n",
    "<!-- >>> your derivation here <<< -->\n",
    "\n",
    "**END Solution**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (1 pt.): Information criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that regression model is\n",
    "$$y = \\sum_{i=1}^k \\beta_i x_i + \\varepsilon,$$\n",
    "and $\\varepsilon$ is dictributed normally: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$, $\\sigma^2$ is known.\n",
    "\n",
    "Prove that the model with highest Akaike information criterion is the model with smallest Mallow's $C_p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "Let $\\hat{y_i}(J) = \\beta_Jx_{iJ}^{\\top}$ is prediction obtained on subset of $J$ features. $m$ is amount of samples.  \n",
    "As it was shown on the lecture, Akaike information cretiria for this model is following:\n",
    "\n",
    "$$L_{J, \\beta_J} =  m\\log \\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{m}(y_i -\\hat{y_i}(J))^2 - |J| $$\n",
    "$$L_{J, \\beta_J} = m\\log \\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{m}(y_i -\\beta_Jx_{iJ}^{\\top})^2 - |J| \\rightarrow \\max\\limits_{\\beta_J, J}$$\n",
    "$$\\frac{2\\sigma^2}{m}L_{J, \\beta_J} = 2\\sigma^2\\log \\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{m}\\sum_{i=1}^{m}(y_i -\\beta_Jx_{iJ}^{\\top})^2 - \\frac{2\\sigma^2}{m}|J| \\rightarrow \\max\\limits_{\\beta_J, J}$$\n",
    "Constant don't ifluence solution of optimization problem. So that the task is equivalent to:\n",
    "$$\\frac{1}{m}\\sum_{i=1}^{m}(y_i -\\beta_Jx_{iJ}^{\\top})^2 + \\frac{2\\sigma^2}{m}|J| \\rightarrow \\min\\limits_{\\beta_J, J}$$\n",
    "\n",
    "Mallow's $C_p$ optimization task for this linear model as it was shown on the lecture is following:\n",
    "\n",
    "$$R(J) =  \\frac{1}{m}\\sum_{i=1}^{m}(y_i -\\hat{y_i}(J))^2 + \\frac{2\\sigma^2}{m}|J| = \\frac{1}{m}\\sum_{i=1}^{m}(y_i -\\beta_Jx_{iJ}^{\\top})^2 + \\frac{2\\sigma^2}{m}|J| \\rightarrow \\min\\limits_{\\beta_J, J}$$\n",
    "\n",
    "The modified problem Akaike and $C_p$ Mallow's problem coincide. It mean that they have coinceding solution and the model with highest Akaike information criterion is the model with smallest Mallow's $C_p$\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting: gradient boosting, adaboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting and its theory\n",
    "\n",
    "Minimization of the loss function is an optimization task, and \"Gradient Boosting\"\n",
    "is one of the many methods to perform optimization. It shoould be noted that it\n",
    "uses *greedy* approach and therefore, like greedy algorithms in CS, may produce\n",
    "results that are not *globally* optimal.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & b_n(x) := \\text{the best base model from the family of the algorithms $\\mathcal{A}$} \\\\\n",
    "    & \\gamma_n(x) := \\text{scale or weight of the new model} \\\\\n",
    "    & a_N(x) = \\sum_{n=0}^N \\gamma_n b_n(x) := \\text{the final composite model}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Algorithm\n",
    "\n",
    "Consider a loss loss function $L(y, z)$ for target $y$ and prediction $z$, and let\n",
    "$(x_i, y_i)_{i=1}^l$ be our train dataset for a regression task. \n",
    "\n",
    "\n",
    "1. Initialize $a_0(x) = \\hat{z}$ with the *flat constant prediction*\n",
    "$\\hat{z} = \\arg\\min\\limits_{z \\in \\mathbb{R}} \\sum_{i=1}^l L(y_i, z)$;\n",
    "2. For $n$ from `1` to `n_boost_steps` do:\n",
    "    * Solve the current subporblem $G_n(b_n, \\gamma_n) \\to \\min\\limits_{b_{n}, \\gamma_n}$\n",
    "    where \n",
    "    $$ G_n(b, \\gamma) = \\sum_{i=1}^l L\\bigl(y_i, a_{n-1}(x_i) + \\gamma b(x_i)\\bigr) $$\n",
    "    with the following method:\n",
    "    \\begin{align}\n",
    "      & s_i = - \\frac{\\partial}{\\partial z} L(y_i, z) \\Big\\vert_{z=a_{n-1}(x_i)}\n",
    "          \\\\\n",
    "      & b_n(x) = \\arg\\min\\limits_{b\\in\\mathcal{A}}\\sum_{i=1}^l \\bigl(b(x_i) - s_i\\bigr)^2\n",
    "          \\\\\n",
    "      & \\gamma_n = \\arg\\min_\\gamma G_n(b_n, \\gamma)\n",
    "          \\\\\n",
    "      & a_n(x) = a_{n-1}(x) + \\gamma_n b_n(x)\n",
    "    \\end{align}\n",
    "3. return $a_N(x) = a_0(x) + \\sum_{n=1}^N \\gamma_n b_n(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (1 pt.)\n",
    "\n",
    "At the $n$-th step of Garient Boosting ($n \\geq 1$) we compute the \"residuals\"\n",
    "$(s_i)_{i=1}^l$ and learn $x\\mapsto b_n(x)$ with a regression algorithm $\\mathcal{A}$\n",
    "applied to the dataset $(x_i, s_i)_{i=1}^l$. For the next two tasks **assume\n",
    "that we have already perfomed these substeps**.\n",
    "\n",
    "Derive the **optimal value** of $\\gamma_n$ for *MSE* loss function $L(y, z) = \\tfrac12 (y - z)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "$$ G_n(b, \\gamma) = \\frac{1}{2}\\sum_{i=1}^l \\bigl(y_i - a_{n-1}(x_i) - \\gamma b(x_i)\\bigr)^2 $$\n",
    "$$ s_i = - \\frac{\\partial}{\\partial z} L(y_i, z) \\Big\\vert_{z=a_{n-1}(x_i)} = (y_i- a_{n-1}(x_i))$$\n",
    " $G_n(b, \\gamma)$ is convex function, so that point, where it achives minimum will be equal to point, where its gradient is equal to zero.\n",
    "$$\\frac{\\partial G_n(b, \\gamma)}{\\partial \\gamma} =  \\sum_{i=1}^l -b(x_i)\\bigl(y_i - a_{n-1}(x_i) - \\gamma b(x_i)\\bigr) = 0  $$\n",
    "$$ \\sum_{i=1}^l y_i - \\sum_{i=1}^l a_{n-1}(x_i) - \\gamma \\sum_{i=1}^l b(x_i) = 0$$\n",
    "$$ \\gamma = \\frac{ \\sum_{i=1}^l y_i - \\sum_{i=1}^l a_{n-1}(x_i)}{\\sum_{i=1}^l b(x_i)} = \\frac{\\sum_{i=1}^l s_i}{\\sum_{i=1}^l b(x_i)}$$\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (1+1+1 pt.)\n",
    "\n",
    "Let $S = (x_i, y_i)_{i=1}^l$ be a sample for a classification task $y_i \\in \\{-1, +1\\}$.\n",
    "\n",
    "The **AdaBoost** algorithm can be regarded as a gradient boosting algorithm\n",
    "with the exponential loss $L(y,z) = e^{-y z}$. Consider the state of **AdaBoost**\n",
    "at the $T$-step\n",
    "$$ G_{T}(b, \\gamma)\n",
    "    = \\sum_{i=1}^l L\\bigl(y_i, a_{T-1}(x_i) + \\gamma b(x_i)\\bigr)\n",
    "    = \\sum_{i=1}^l \\underbrace{\\exp(- y_i a_{T-1}(x_i))}_{w^{T-1}_i}\n",
    "        \\exp(- y_i \\gamma b(x_i))\n",
    "    \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.1 (1 pt.)\n",
    "\n",
    "Derive the next weights $w^T_i$ used in $G_T$ at the stage $T$ as a function\n",
    "of the learnt classifier $b_T$ and the current weights $w^{T-1}_i$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution\n",
    "\n",
    "Consider the situation, when optimal $b_T$ and $\\gamma$ were found. On the next stage $T+1$ we have new classifer $c_{T+1}(x)$. Then\n",
    "$$ G_{T+1}(c, \\beta)\n",
    "    = \\sum_{i=1}^l L\\bigl(y_i, a_{T-1}(x_i) + \\gamma b(x_i)+ \\beta c(x_i)\\bigr)\n",
    "    = \\sum_{i=1}^l \\underbrace{w^{T-1}_i\n",
    "        \\exp(- y_i \\gamma b(x_i))}_{w^T_i} \\exp(- y_i \\beta c(x_i)) = \\sum_{i=1}^l w^T_i \\exp(- y_i \\beta c(x_i))$$\n",
    "$$ w^T_i = w^{T-1}_i \\exp(- y_i \\gamma b(x_i)) $$\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.2 (1 pt.)\n",
    "\n",
    "Compute the ratio of weights $(w^T_i)_{i=1}^l$ between the normal and outlier\n",
    "samples in $S$. Propose a **formal definition of being an outlier**, and reflect\n",
    "on the value of *margin* for both.\n",
    "\n",
    "<span style=\"color:green\">**HINT**</span>: *margin* value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution\n",
    "\n",
    "$b_T$ is algorithm learnt on step $T$.\n",
    "\n",
    "Let $x_i$ is normal sample. Then $w^T_i = \\exp(-y_ia_T(x_i))$ decreases exponentially with rising $T$, because $sing(y_i) = sign(b_T(x_i))$. \n",
    "\n",
    "Let $x_j$ is outlier sample. It means that on each iteration $w^T_j$ should increases due to multiplication by $\\exp(-y_jb_T(x_j))$ on each step, because $sign(y_j) \\ne sign(b_T(x_j))$.\n",
    "\n",
    "So that I propose the following defenition: $x_j$ is outlier if $\\frac{1}{l-1} \\sum\\limits_{i\\ne j} w^T_i \\ll w^T_j$.\n",
    "\n",
    "About margin I can say, that we can define margin as $\\frac{y_i\\sum\\limits_{t=1}^{T}\\alpha_th_t(x_j)}{\\sum\\limits_{t=1}^{T}|\\alpha_t|}$, where $h_t(x)$ - base classifiers. If the margin is negative then the object is misclassified and if we consider weight of this object, it should be much bigger than for other objects from particular class. It mean that misclassified object is outlier. For normal object weights are small and margins shuold be positive.\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.3 (1 pt.)\n",
    "\n",
    "Conclude about **sensitivity** of Adaboost to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution\n",
    "\n",
    "Adaboost has high sensitivity to outliers. Exponential loss function significantly increases weights of the most difficult objects. That's why AdaBoost has tend to overfit when data has significant level of noise. AdaBoost starts to fit to noise that result in overfitting. This problem could be solved by outliers' removing from intial data. But then the following problem appers: how to classify ouliers.\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (2+1+2 pt.): Alternative objective functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem studies boosting-type algorithms defined with objective\n",
    "functions different from that of AdaBoost.We assume that the training\n",
    "data are given as m labeled examples $(x_{1},y_{1}),...,(x_{m},y_{m}) \\in X \\times \\{-1,+1\\}$.We further assume that $\\Phi$ is a strictly increasing convex and differentiable function over $\\mathbb{R}$ such that:$\\ \\forall x \\geqslant 0,\\Phi(x)\\geqslant 1 \\ and \\ \\forall x < 0,\\Phi(x) > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.1 (2 pt.)\n",
    "Consider the loss function $L(a) =\\sum_{i=1}^{m}\\Phi \\left( -y_{i}g(x_i) \\right)$ where $g$ is a linear combination of base classifiers, i.e., $g= \\sum_{t=1}^{T} a_t h_t$(as in\n",
    "AdaBoost). Derive a new boosting algorithm using the objective function $L$. In particular, characterize the best base classifier $h_u$ to select at each round of boosting if we use coordinate descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "$$h_u = arg\\min\\limits_{h_u \\in H} \\sum_{i=1}^{m}\\Phi \\left(-y_{i}(g(x_i) - \\gamma h_u(x_i))\\right)$$\n",
    "We use coordinate descent, it means that we should minimize objective function along one direction. This direction is defined by coeffecient $\\gamma$.\n",
    "$$\\frac{\\partial \\sum_{i=1}^{m}\\Phi \\left(-y_{i}(g(x_i) - \\gamma h_u(x_i))\\right)}{\\partial \\gamma}\\Bigg|_{\\gamma=0}=\n",
    "-\\sum\\limits_{i=1}^m y_ih_u(x_i)\\Phi'(-y_ig(x_i)) \\propto -\\sum\\limits_{i=1}^m y_ih_u(x_i)\\frac{\\Phi'(-y_ig(x_i))}{\\sum\\limits_{i=1}^m \\Phi'(-y_ig(x_i))} = -\\sum\\limits_{i=1}^m y_ih_u(x_i)\\widetilde{w}_i^{T}.$$\n",
    "In such way weights were defined in AdaBoost. Let's assume that $h_u$ has values $1$ or $-1$. Then \n",
    "$$\\sum\\limits_{i=1}^m y_ih_u(x_i)\\widetilde{w}_i^{T}=1-2\\sum\\limits_{i: h(x_i) \\ne y_i}^m \\widetilde{w}_i^{T} = 1-2\\epsilon_u, $$ where $\\epsilon_u =\\mathbb{P}(y_i\\ne h_u(x_i))$ and $h_u$ minimizes $\\epsilon_u$. At each round we select the base classifier $h_u$ with the minimal probability of error on training data.\n",
    "\n",
    "Then we can choose coefficinets gamma in the following way: $$ \\gamma^* = arg\\min\\limits_{\\gamma} \\sum_{i=1}^{m}\\Phi \\left(-y_{i}(g(x_i) + \\gamma h_u(x_i))\\right)$$\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.2 (1 pt.)\n",
    "Consider the following functions:  \n",
    "\n",
    "1. zero-one loss $\\Phi_1(−u) = 1_{u\\leqslant0}$;  \n",
    "2. least squared loss $\\Phi_2(−u) = (1 − u)^2$;  \n",
    "3. SVM loss $\\Phi_3(−u) = \\max\\{0, 1 − u\\}$;  \n",
    "4. logistic loss $\\Phi_4(−u) = \\log(1 + e^{−u})$.  \n",
    "\n",
    "Which functions satisfy the assumptions on $\\Phi$ stated earlier in this\n",
    "problem?\n",
    "\n",
    "Compute the gradient for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "1. zero-one loss $\\Phi_1(−u) = 1_{u\\leqslant0}$ is not differentiable and not convex - don't satisfy\n",
    "2. least squared loss $\\Phi_2(−u) = (1 − u)^2$ is not striclty increasing function - don't satisfy\n",
    "3. SVM loss $\\Phi_3(−u) = \\max\\{0, 1 − u\\}$ is equal to zero if $-u \\leq -1$ and not differetiable in point 1\n",
    "4. logistic loss $\\Phi_4(−u) = \\log_2(1 + e^{−u})$, $\\Phi_4(x) = \\log_2(1 + e^{x})$ satisfies all assumption\n",
    "\n",
    "$$ \\nabla \\Phi_4(x) = \\nabla \\log_2(1 + e^{x}) = \\nabla \\frac{\\ln(1+e^x)}{\\ln 2} = \\frac{1}{\\ln 2} \\frac{e^x}{1+e^x} = \\frac{1}{\\ln 2} \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.3* (2 pt.)\n",
    "For each loss function satisfying the assumptions in Task 4 formualtion, derive the\n",
    "corresponding boosting algorithm. How do the algorithm(s) differ\n",
    "from AdaBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "\n",
    "$\\Phi(x) = \\log_2(1 + e^{x})$\n",
    "\n",
    "In task 4.1 it was shown that wieghts of each sample can be calculated using gradient of loss function.\n",
    "Thus, $h(x)$ should maximize $\\sum\\limits_{i=1}^{m}\\frac{y_ih_t(x_i)}{1+\\exp(y_ig(x_i))}$. Let's consider \n",
    "$\\sum\\limits_{i=1}^{m}\\Phi(-y_i(g(x_i)+\\alpha h(x_i))) - \\Phi(-y_ig(x_i)) = \\sum\\limits_{i=1}^{m}\\log_2 \\left(\\frac{1+\\exp(-y_i(g(x_i)+\\alpha h(x_i)))}{1+\\exp(-y_ig(x_i))} \\right) = \\sum\\limits_{i=1}^{m}\\log_2 \\left(1+\\frac{\\exp(-y_i(g(x_i)+\\alpha h(x_i)))-\\exp(-y_ig(x_i))}{1+\\exp(-y_ig(x_i))}\\right) \\leq \\sum\\limits_{i=1}^{m}\\frac{1}{\\ln2} \\frac{\\exp(-y_i(g(x_i)+\\alpha h(x_i)))-\\exp(-y_ig(x_i))}{1+\\exp(-y_ig(x_i))} = \\frac{1}{\\ln2} \\sum\\limits_{i=1}^{m} \\frac{\\exp(-y_i\\alpha h(x_i))-1}{1+\\exp(y_ig(x_i))}$\n",
    "\n",
    "It means that to calculate weights we can take logarithmic loss function. And to minimize loss function on each next iteration we should minimize $\\sum\\limits_{i=1}^{m} \\frac{\\exp(-y_i\\alpha h(x_i))-1}{1+\\exp(y_ig(x_i))}$\n",
    "\n",
    "\n",
    "The intial classifier $h_0(x) \\equiv 0$\n",
    "\n",
    "For $t=1\\dots T$\n",
    "\n",
    "1. Calculate new weights $\\widetilde{w}_i^{t} = \\frac{1}{Z} \\frac{1}{1+\\exp(y_ig(x_i))}$, where $g(x_i)=\\sum\\limits_{j=0}^{t-1}\\alpha_j h_j(x_i)$, $Z = \\sum\\limits_{i=1}^{m} \\frac{1}{1+\\exp(y_ig(x_i))}$\n",
    "\n",
    "2. Choose $\\alpha_t \\in R$, new base classifier $h_t(x) \\in H$, such that the minimize the loss function $\\sum\\limits_{i=1}^{m} \\widetilde{w}_i^{t} \\exp(-y_i\\alpha_th_t(x_i))$\n",
    "\n",
    "3. $g(x) = \\sum\\limits_{j=0}^{t-1} \\alpha h_j(x) + \\alpha_t h_t(x)$\n",
    "\n",
    "From AdaBoost this algorithm differs in calculation of weights (using logarithmic loss instead of exponential). Also in AdaBoost loss from previous iterations is multiplied by current loss. In this case current loss is added to previous one.\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. (1 pt.)\n",
    "Consider a two-layer network function of the form\n",
    "    \\begin{equation}\n",
    "    y_k(x, \\mathbf{w}) = \\sigma \\left ( \\sum_{j=1}^M w_{kj}^{(2)} \\sigma \\left(\\sum_{i=1}^D w_{ji}^{(1)}x_i + w_{j0}^{(1)}\\right)\n",
    "                               + w_{k0}^{(2)}\\right)\n",
    "    \\end{equation}\n",
    "in which the nonlinear activation functions are logistic sigmoid functions\n",
    "    \\begin{equation}\n",
    "    \\sigma(a) = (1 + \\exp(−a))^{-1}.\n",
    "    \\end{equation}\n",
    "Show that there exists an equivalent network, which computes exactly the same function but with hidden unit activation function given by hyperbolic tangent ${\\rm tanh}(a)$\n",
    "    \\begin{equation}\n",
    "    {\\rm tanh}(a) = \\frac{e^a - e^{-a}}{e^a + e^{-a}}.\n",
    "    \\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "Let's consider the following equation:\n",
    "$${\\rm tanh}(a) + 1 = 1 + \\frac{e^{a} - e^{-a}}{e^{a} + e^{-a}} = \\frac{2e^a}{e^a + e^{-a}} = \\frac{2e^a e^{-a}}{(e^a + e^{-a})e^{-a}}=\\frac{2}{1 + e^{-2a}} = 2 \\sigma(2a)$$.\n",
    "\n",
    "Then we can rewrite $\\sigma(a)$ via ${\\rm tanh}(a) $:\n",
    "\n",
    "$$\\sigma(a) = \\frac{{\\rm tanh}(\\frac{a}{2}) + 1}{2}$$  \n",
    "\n",
    "Apply this to the hidden layer:\n",
    "$$     y_k(x, \\mathbf{w}) = \\sigma \\left ( \\sum_{j=1}^M w_{kj}^{(2)} \\sigma \\left(\\sum_{i=1}^D w_{ji}^{(1)}x_i + w_{j0}^{(1)}\\right)\n",
    "                               + w_{k0}^{(2)}\\right) =  \\sigma \\left ( \\sum_{j=1}^M w_{kj}^{(2)} \\frac{{\\rm tanh}\\left[\\frac{\\left(\\sum_{i=1}^D w_{ji}^{(1)}x_i + w_{j0}^{(1)}\\right)}{2}\\right] + 1}{2} +  \n",
    "                               w_{k0}^{(2)}\\right) $$\n",
    "\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6*. Data augmentation (2 pt.)\n",
    "One way to encourage invariance of a model w.r.t a set of transformations is to expand the training set using transformed versions of the original input patterns.\n",
    "Consider the framework for training with transformed data in the special case when the transformation is simply addition of random noise $x \\rightarrow x + \\xi$ where $\\xi$ has a Gaussian distribution with zero mean and unit variance. The error function for untransformed inputs can be written (in the infinite data set limit) in the form\n",
    "    \\begin{equation}\n",
    "    E = \\frac12 \\int \\int (y(\\mathbf{x}) - t)^2 p(t|\\mathbf{x}) p(\\mathbf{x}){\\rm d}\\mathbf{x} {\\rm d}t.\n",
    "    \\end{equation}\n",
    "If we now consider an infinite number of copies of each data point perturbed by the transformation, then the error function can be written as\n",
    "    \\begin{equation}\n",
    "    \\widetilde{E} = \\frac12 \\int\\int(y(x + \\xi) - t)^2p(t | \\mathbf{x})p(\\mathbf{x}) p(\\xi){\\rm d}\\mathbf{x}{\\rm d}t {\\rm d}\\xi\n",
    "    \\end{equation}\n",
    "Using Taylor series expansion of $y(\\mathbf{x} + \\xi)$ show that\n",
    "    \\begin{equation}\n",
    "    \\widetilde{E} \\simeq E + \\lambda \\Omega\n",
    "    \\end{equation}\n",
    "where $\\Omega$ is a regularizer which takes the form of Tikhonov regularizer\n",
    "    \\begin{equation}\n",
    "    \\Omega = \\frac12 \\int \\|\\nabla y(\\mathbf{x})\\|^2 p(\\mathbf{x}){\\rm d} \\mathbf{x}.\n",
    "    \\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "$y(x+\\xi) \\approx  y(x)+(\\nabla y(x), \\xi) \\approx y(x) + \\|\\nabla y(\\mathbf{x})\\| |\\xi|$, because $|(\\nabla y(x), \\xi)| \\leq \\|\\nabla y(x)\\| \\xi$\n",
    "\n",
    "$$ (y(x + \\xi) - t)^2 \\approx  (y(x)-t)^2 + 2(y(x)-t)(\\nabla y(x), \\xi) + (\\nabla y(x), \\xi)^2 = (y(x)-t)^2 + 2(y(x)-t) (\\nabla y(x), \\xi) + \\|\\nabla y(\\mathbf{x})\\|^2 \\xi^2$$\n",
    "\n",
    "\n",
    "$$\\widetilde{E} \\approx \\frac12 \\int\\int\\int(y(x)-t)^2 + 2(y(x)-t)(\\nabla y(x), \\xi) + \\|\\nabla y(\\mathbf{x})\\|^2 \\xi^2p(t | \\mathbf{x})p(\\mathbf{x}) p(\\xi){\\rm d}\\mathbf{x}{\\rm d}t {\\rm d}\\xi$$\n",
    "\n",
    "In three obtained integrals we can divide variable. The first element of sum after integrating over $\\xi$ will be equal to $E$, because $\\int p(\\xi) {\\rm d} \\xi = 1$\n",
    "\n",
    "The second componet will be equal to zero, because after integrating over $\\xi$, $\\int \\xi p(\\xi) {\\rm d} \\xi = 0$, because mean value of $\\xi$ is equal to zero due to intial condition.\n",
    "\n",
    "And after integrating the third component we obtain the following:\n",
    "$$\\frac12 \\int\\int\\int\\|\\nabla y(\\mathbf{x})\\|^2 \\xi^2p(t | \\mathbf{x})p(\\mathbf{x}) p(\\xi){\\rm d}\\mathbf{x}{\\rm d}t {\\rm d}\\xi = \\frac12 \\int\\int \\|\\nabla y(\\mathbf{x})\\|^2 p(x)p(t|x) {\\rm d} x {\\rm d} t \\int \\xi^2p(\\xi) {\\rm d} \\xi$$\n",
    "\n",
    "Due to Bayes formula $p(x)p(t|x)$ = $p(t)p(x|t)$. $\\int \\xi^2p(\\xi) {\\rm d} \\xi = 1$ - due to unit variance. Finally,\n",
    "\n",
    "$$\\frac12 \\int\\int \\|\\nabla y(\\mathbf{x})\\|^2 p(x)p(t|x) {\\rm d} x {\\rm d} t = \\frac12 \\int \\|\\nabla y(\\mathbf{x})\\|^2 \\int p(t)p(x|t)  {\\rm d} t {\\rm d} x = \\frac12 \\int \\|\\nabla y(\\mathbf{x})\\|^2 p(x) {\\rm d} x = \\Omega$$\n",
    "\n",
    "And final formula is true, due to approximation via Taylor series. $\\widetilde{E} \\simeq E + \\lambda \\Omega$\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
