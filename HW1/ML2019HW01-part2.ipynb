{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 1: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem just right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "* The are two problems with \\* mark - they are not obligatory. You can get **EXTRA POINTS** for solving them.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Linear Ridge Regression (1 point)\n",
    "Let us consider the problem of linear ridge regression for data $(x_{1},y_{1}),\\dots,(x_{n},y_{n})\\in\\mathbb{R}^{d\\times 1}$. Let the objects have positive **sample weights** $v_{i}>0$, i.e. the optimization problem is\n",
    "$$\\sum_{i=1}^{n}v_{i}\\cdot L(y_{i}, \\hat{y}_{i})+\\frac{\\lambda}{2}\\|w\\|_{2}^{2}=\\sum_{i=1}^{n}v_{i}\\cdot (\\langle\\boldsymbol{w},\\boldsymbol{x}_{i}\\rangle-y_{i})^{2}+\\frac{\\lambda}{2}\\|w\\|_{2}^{2}\\rightarrow \\min_{\\boldsymbol{w}}.$$\n",
    "This problem reduces to classical linear ridge regression when $v_{i}\\equiv 1$. The matrix form of the problem is\n",
    "$$(Xw-y)^{\\top}V(Xw-y)+\\frac{\\lambda}{2}w^{\\top}w\\rightarrow\\min_{w},$$\n",
    "where $V=V^{\\top}\\in\\mathbb{R}^{n\\times n}$ is the diagonal matrix with diagonal elements $v_{1},\\dots, v_{n}$. Note that the quadratic problem is still convex (w.r.t. $\\boldsymbol{w}$), thus, the solution is unique. Solve this problem for any (symmetric) positive-definite matrix $V$ (not just diagonal) and provide the answer in the matrix form.\n",
    "### Your solution:\n",
    "\n",
    "   $$ f(w) = (Xw-y)^{\\top}V(Xw-y)+\\frac{\\lambda}{2}w^{\\top}w = \\langle Xw-y,V(Xw-y) \\rangle+\\frac{\\lambda}{2} \\langle w,w \\rangle $$\n",
    "   \n",
    "   $$ df(w) = \\langle Xdw, V(Xw-y) \\rangle +\\langle (Xw-y), VXdw \\rangle +\\frac{\\lambda}{2} \\langle w,dw \\rangle+\\frac{\\lambda}{2} \\langle dw,w\\rangle = $$\n",
    "   $$ =  \\langle X^{\\top}V(Xw-y), dw\\rangle+ \\langle (VX)^{\\top}(Xw-y), dw\\rangle+\\lambda \\langle w, dw\\rangle =  \\langle X^{\\top}V(Xw-y), dw\\rangle+ \\langle X^{\\top}V(Xw-y), dw\\rangle+\\lambda \\langle w, dw\\rangle = $$\n",
    "   $$ = 2 \\langle X^{\\top}V(Xw-y), dw\\rangle+\\lambda \\langle w, dw\\rangle$$\n",
    "   \n",
    "   $$ \\nabla_w f(w) = 2X^{\\top}V(Xw-y)+\\lambda w =0$$\n",
    "   \n",
    "   $$ (2X^{\\top}VX+\\lambda I)w = 2X^{\\top}Vy$$\n",
    "   $$ w = (2X^{\\top}VX+\\lambda I)^{-1} 2X^{\\top}Vy$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Logistic Regression (1 point)\n",
    "Let us consider the case when in the problem of binary classification the training set is lineary separable. Show that in this case the optimization problem for logistic regression **without L2-regularization** does not have optimum.\n",
    "### Your solution: \n",
    "\n",
    "The loss fuction for logistic regression without L2-regularization and lineary separable trainig set is $L = \\sum_{i=1}^{m}log(1+\\exp(-(w^{\\top}x_i)y_i)$. By contraddiction, if it has optimum then its gradient in optimum point is equal to zero.\n",
    "\n",
    "$$\n",
    "\\nabla_w L = -\\sum_{i=1}^{m} \\frac{\\exp(-(w^{\\top}x_i)y_i)}{1+\\exp(-(w^{\\top}x_i)y_i)}y_ix_i = 0\n",
    "$$\n",
    "Vectors $x_i$ are independent and identically-distributed, then their linear combination is equal zero iff all coeffecients are equal to zero. But $\\frac{\\exp(-(w^{\\top}x_i)y_i)}{1+\\exp(-(w^{\\top}x_i)y_i)}y_i \\ne 0$ for any $i$ and $w$. Consequently, loss function doesn't have optimum by contradiction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Bayesian Naive Classifier (1 point)\n",
    "Show that in case of $d$ binary-valued features $x_{j}\\in\\{0, 1\\}$ (for $j=1,2,\\dots,d$) Bayesian Naive Classifier's decision rule is linear.\n",
    "### Your solution:\n",
    "\n",
    "Decision rule for Bayesian Naive Classifier is $\\hat{y} = \\arg \\max\\limits_{k \\in [1, \\dots, K]}p(C_k)\\prod_{i=1}^{d}p(x_i|C_k)$. If each feature is equal to one or zero, then they have Bernoulli distribution. In this case decidion rule is $\\hat{y} = \\arg \\max\\limits_{k \\in [1, \\dots, K]}p(C_k)\\prod_{i=1}^{d}p_{ki}^{x_i}(1-p_{ki})^{(1-x_i)}$, where $p_{ki} = P(x_i=1|y_i \\in C_i)$. Logarithm doesn't change point of optimum, then apply logarithmic scale to decision rule. \n",
    "\n",
    "$\\hat{y} = \\arg \\max\\limits_{k \\in [1, \\dots, K]}log[p(C_k)\\prod_{i=1}^{d}p_{ki}^{x_i}(1-p_{ki})^{(1-x_i)}] = \\arg \\max\\limits_{k \\in [1, \\dots, K]} log(p(C_k))+ \\sum_{i=1}^{d}[x_ilog(p_{ki})+(1-x_i)log(1-p_{ki})]$. \n",
    "\n",
    "This rule is linear with weights that are equal to $w_i=log(p_{ki})$ if $x_i=1$ or $w_i=log(1-p_{ki})$ if $x_i=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Nearest Neighbors (1 point)\n",
    "Consider the 1-nearest-neighbor classifier applied to multiclass classification problem. Let's denote the classifier fitted on data $X$ by $f_X(\\cdot)$.\n",
    "\n",
    "The formula for complete **leave-k-out cross-validation** on data sample $X^{n}$ is defined as\n",
    "$$L_{k}OCV=\\frac{1}{C_{n}^{k}}\\bigg[\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg)\\bigg],$$\n",
    "where $\\mathcal{P}(X^{n})$ is the set of all subsets of $X^{n}$. For all $i=1,2\\dots,n$ denote the label of $m$-th closest neighbor of $x_{i}$ in $X^{n}\\setminus \\{x_{i}\\}$ by $y_{i}^{m}$. Show that \n",
    "$$L_{k}OCV=\\sum_{m=1}^{k}\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq y_{i}^{m}]}_{K_{m}(X^{n})}\\frac{C_{n-1-m}^{n-k-1}}{C_{n-1}^{k-1}},$$\n",
    "where $K_{m}(X^{n})$ is called the compactness profile of $X^{n}$, i.e. the fraction of objects whose $m$-th nearest neighbor has different label. For convenience assume that all the pairwise distances between objects are different.\n",
    "### Your solution:\n",
    "\n",
    "The probabailty that the $m$ nearest heighbors of $y_{i}$  are not included in subset of size k is $P(y_{i}^1, \\dots y_{i}^m \\notin X) = \\frac{C_{n-k-1}^{n-m-1}}{C_{n-1}^{k-1}}$, where $C_{n-1}^{k-1}$ - the number of all subsets of size $k-1$ that don't include $y_i$,  $C_{n-k-1}^{n-m-1}$ - the number of all subsets of size $k-1$ that don't include $m$ nearest neighbors of $y_i$ in training samples.\n",
    "\n",
    "Then the error in leave-k-out cross-validation can be calculated as the sum of error for each object. And the error for each object can be calculated as the sum of cases when $m$-th nearest neighbors included in test subset but the prediction is wrong.\n",
    "\n",
    "Then $L_{k}OCV = \\frac{1}{n}\\sum_{i=1}^{n}\\sum_{m=1}^{k}[y_i \\ne y_i^m]\\frac{C_{n-k-1}^{n-m-1}}{C_{n-1}^{k-1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Bootstrap (1 point)\n",
    "Let the subsample $\\hat{X}^{n}$ of size $n$ be generated from sample $X^{n}=\\{\\boldsymbol{x}_{1},\\dots\\boldsymbol{x}_{n}\\}$ by bootstrap procedure. Find the probability of object $x_{i}$ to be present in $\\hat{X}^{n}$ and compute the limit of this probability for $n\\rightarrow\\infty$.\n",
    "### Your solution:\n",
    "\n",
    "The probalitity that objest $x_i$ will not be included in subset of $n$ elemetns is $P(x \\notin \\hat{X}^{n}) = \\left(\\frac{n-1}{n}\\right)^{n}$. Consequently, $P(x \\in \\hat{X}^{n})  = 1-\\left(\\frac{n-1}{n}\\right)^{n}$. \n",
    "$$\n",
    "\\lim _{n\\rightarrow\\infty} 1-\\left(1-\\frac{1}{n}\\right)^{n}=1 -\\frac{1}{e} = \\frac{e-1}{e}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. Decision Tree Leaves (1+1+1=3 points)\n",
    "Consider a leaf of a binary decision tree which consists of object-label pairs $(x_{1},y_{1}),\\dots,(x_{n},y_{n})$. The prediction $\\hat{y}$ of this leaf is defined to minimize the loss on the training sample, i.e. $\\frac{1}{n}\\sum_{i=1}^{n}L(y_{i}, \\hat{y})\\rightarrow\\min$. We consider three cases:\n",
    "1. Regression tree ($y_{i}\\in\\mathbb{R}$), absolute loss function $L(y,\\hat{y})=|y-\\hat{y}|$. The optimal prediction that minimizes $\\frac{1}{n}\\sum_{i=1}^{n}|y_{i}-\\hat{y}|$ is the median of labels: \n",
    "$$\\hat{y}=\\text{median}(y_{1},\\dots,y_{n}).$$\n",
    "In this case, for simplicity you may assume that $n$ is even (or odd, as you wish).\n",
    "2. Regression tree ($y_{i}\\in\\mathbb{R}$), squared loss function $L(y,\\hat{y})=(y-\\hat{y})^{2}$. The optimal prediction that minimizes $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\hat{y})^{2}$ is the mean of labels:\n",
    "$$\\hat{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_i.$$\n",
    "3. Classification tree for K classes ($y_{i}\\in\\{1,2,\\dots,K\\}$), zero-one loss $L(y,\\hat{y})=[y\\neq \\hat{y}]$. The optimal prediction that minimizes $\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq\\hat{y}]$ is the most frequent label:\n",
    "$$\\hat{y}=\\underset{k=1,2,\\dots,K}{\\operatorname{argmax}}\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=k].$$\n",
    "In this case, for simplicity you may assume that there is only one most frequent label.\n",
    "\n",
    "Suppose that that instead of using optimal prediction for this leaf we output the label from $y_{1},\\dots,y_{n}$ at random. What will happen with the (expected) loss on the training sample (will it increase/decrease/not change)? Prove your answer (separately for every case).\n",
    "### Your solution:\n",
    "\n",
    "Oprimal prediction is chosen from uniform distribution, then $P(\\hat{y}=y_i) = \\frac{1}{n}$.\n",
    "\n",
    "1. Suppose that $y_k = \\text{median}(y_{1},\\dots,y_{n})$. Then $L(y, y_k) = \\frac{1}{n}\\sum_{i=1}^{n}|y_{i}-y_k| = \\min\\limits_{j \\in [1, \\dots, n]}L(y, y_j).$ \n",
    "$$ E(L) = \\frac{1}{n^2}\\sum_{i=1}^{n} \\sum_{j=1}^{n}|y_{i}-y_j| = \\frac{1}{n}\\sum_{j=1}^{n}L(y, y_j) \\geq  \\frac{1}{n}\\sum_{j=1}^{n}L(y, y_k) = L(y, y_k)$$\n",
    " Expected loss increases in case of random prediction.\n",
    "\n",
    "2. $$L_0 = L(y, \\hat{y}) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(y_{i}-\\frac{1}{n}\\sum_{i=j}^{n}y_j\\right)^{2} = \\frac{1}{n} \\sum_{i=1}^{n}\\left(y_i^2-2\\frac{y_i}{n}\\sum_{j=1}^{n}y_j+\\frac{1}{n^2}\\left(\\sum_{j=1}^{n}y_i\\right)^2\\right) = \\frac{1}{n}\\sum_{i=1}^{n} y_i^2 - \\frac{2}{n^2}\\sum_{i=1}^{n} y_i \\sum_{j=1}^{n} y_j + \\frac{1}{n^2}\\left(\\sum_{j=1}^{n}y_i\\right)^2$$\n",
    "\n",
    "$$E(L) =  \\frac{1}{n^2}\\sum_{i=1}^{n} \\sum_{j=1}^{n}(y_{i}-y_j)^2 = \\frac{1}{n^2}\\sum_{i=1}^{n} \\sum_{j=1}^{n}(y_i^2-2y_iy_j+y_j^2) = \\frac{1}{n}\\sum_{i=1}^{n}y_i^2 - \\frac{2}{n^2}\\sum_{i=1}^{n} y_i \\sum_{j=1}^{n} y_j +\\frac{1}{n} \\sum_{j=1}^{n}y_j^2$$\n",
    "\n",
    "$$E(L)-L_0 = \\frac{1}{n} \\sum_{j=1}^{n}y_j^2 - \\frac{1}{n^2}\\left(\\sum_{j=1}^{n}y_i\\right)^2 \\geq 0$$\n",
    " Expected loss increases in case of random prediction.\n",
    "\n",
    "3. $k$ - most frequent label and unique. \n",
    "$L_0 = L(y, k) = \\frac{1}{n} \\sum_{i=1}^{n}[y_i \\ne k] = \\min\\limits_{j \\in [1, \\dots, K]}L(y, j)$\n",
    "\n",
    "$$E(L) = \\frac{1}{K}\\sum_{j=1}^{K} \\frac{1}{n}\\sum_{i=1}^{n}[y_i \\ne j] \\geq \\frac{1}{K}\\sum_{j=1}^{K} \\frac{1}{n}\\sum_{i=1}^{n}[y_i \\ne k] = \\frac{1}{K}\\sum_{j=1}^{K} L(y, k) = L(y,k)=L_0$$\n",
    "\n",
    " Expected loss increases in case of random class prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7*. Classification (1 point)\n",
    "Let objects $\\boldsymbol{x}_{1},\\dots,\\boldsymbol{x}_{n}$ have binary labels $y_{1}, y_{2},\\dots,y_{n}\\in\\{0,1\\}$. Let the classifier $f$ assign to objects probabilities of being from class $1$. Without loss of generality assume that $f(\\boldsymbol{x_{i}})<f(\\boldsymbol{x_{j}})$ for all $i<j$. Denote the number of objects of $X^{n}$ from class $1$ by $n_{1}=\\sum_{i=1}^{n}[y_{i}=1]$. Show that \n",
    "$$S_{\\text{ROC}}(f,X^{n})=\\frac{1}{n_{1}(n-n_{1})}\\sum_{i<j}[y_{i}<y_{j}]$$\n",
    "where $S_{\\text{ROC}}(f,X^{n})$ is the Area Under ROC of classifier $f$.\n",
    "### Your solution:\n",
    "\n",
    "$t$ - is probability treshhold.\n",
    "$S_{ROC} = \\int_{0}^{1}TPR d(FPR)$, where $TPR = \\frac{\\sum_{i=1}^{n_1}[p(x_i)>t|y(x_i)=1]}{n_1}$ and $FPR = \\frac{\\sum_{j=1}^{n-n_1}[p(x_j)>t|y(x_j)=0]}{n-n_1}$\n",
    "\n",
    "$$S_{ROC} = \\sum_{i=1}^{n_1}\\sum_{j=1}^{n-n_1}\\frac{[p(x_i)>t|y(x_i)=1]}{n_1}\\frac{[p(x_j)>t|y(x_j)=0]}{n-n_1} =\n",
    " = \\frac{1}{n_1(n-n_1)} \\sum_{i=1}^{n_1}\\sum_{j=1}^{n-n_1} [p(x_i)>p(x_j)|y(x_i)=1 \\wedge  y(x_j) = 0] =  \\frac{1}{n_1(n-n_1)}\\sum_{i=1}^{n_1}\\sum_{j=1}^{n-n_1} [y_i>y_j]$$\n",
    "And due to intial assumption: $S_{ROC} = \\frac{1}{n_1(n-n_1)}\\sum_{i=1}^{n_1}\\sum_{j=1}^{n-n_1} [y_i>y_j] = \\frac{1}{n_{1}(n-n_{1})}\\sum_{i<j}[y_{i}<y_{j}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8. Kernel Regression (1 point)\n",
    "Recall that the prediction of Kernel Ridge Regression fitted on data $X^{n}$ with the kernel $K(\\cdot, \\cdot)$ has the form $\\mathcal{K}(x)=\\sum_{i=1}^{n}\\alpha_{i}K(x, x_{i})$, where $\\alpha=(K+\\lambda I)^{-1}Y$ ($K_{ij}=K(x_{i},x_{j})$). The time complexity of computation of a prediction $\\mathcal{K}(x)$ for any point $x$ is $O(n)$, i.e. grows linearly with the size of the training set.\n",
    "\n",
    "Consider the bilinear kernel $K(x, x')=\\langle x, x'\\rangle$. For this kernel, the Kernel Regression is known to turn into simple linear ridge regression. However, for linear regression the computation time of prediction $\\mathcal{R}(x)=\\sum_{j=1}^{d}\\beta_{j}x^{j}$ is $O(d)$, where $d$ is the dimension of the feature space and does not grow with the training, which is a little bit controversary to the previous paragraph.\n",
    "\n",
    "In this task in order to show that the kernel regression with the bilinear kernel is indeed the linear ridge regression, you have to prove that the predictions exactly match by **direct comparison**.\n",
    "### Your solution:\n",
    "\n",
    "If kernel is $K(x, x')=\\langle x, x'\\rangle$, then matrix $K = XX^{\\top}$, where $X$ is matrix of samples that has size $n\\times d$, $n$ - number of samples, $d$ - number of features. Vector $\\alpha = (XX^{\\top}+\\lambda I)^{-1}Y$ has size $n\\times 1$. Then prediction $Y_{pred} = K \\alpha = XX^{\\top}(XX^{\\top}+\\lambda I)^{-1}Y = X(X^{\\top}X+\\lambda I)^{-1}X^{\\top}Y$.\n",
    "\n",
    "Ridge regression optimizes the following functional: $ F(w) = \\|Xw-Y\\|_2^2 + \\frac{\\lambda}{2}\\|w\\|_{2}^{2}\\rightarrow \\min_{\\boldsymbol{w}}$. It gives answer like in first task if $V=I$, $w = (X^{\\top}X+\\lambda I)^{-1}X^{\\top}Y$ and prediction $Y_{pred} = X(X^{\\top}X+\\lambda I)^{-1}X^{\\top}Y$ that coincides with prediction obtained by Kernel regression with kernel $K(x, x')=\\langle x, x'\\rangle$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9. Kernel Methods (1 point)\n",
    "Prove that the function $K(x,x')=\\exp(-\\|x-x'\\|^{2})$ is positive definite kernel.\n",
    "### Your solution: \n",
    "\n",
    "Matrix $K_{ij} = \\exp(-\\|x_i-x_j\\|^{2}) = \\exp(-\\|x_j-x_i\\|^{2}) = K_{ji}$ - symmetric. \n",
    "\n",
    "$ \\sum_{i=1}^{n}\\sum_{j=1}^{n}c_ic_jK_{ij} =   \\sum_{i=1}^{n}\\sum_{j=1}^{n}c_ic_j\\exp(-\\|x_i-x_j\\|^{2}) = \\sum_{i=1}^{n}\\sum_{j=1}^{n}c_ic_j\\exp(-\\|x_i\\|^{2})\\exp(-\\|x_j\\|^{2}) \\exp(2x_i^{\\top}x_j)$.\n",
    "\n",
    "Let's denote $c_i\\exp(-\\|x_i\\|^{2}) = a_i$ and $c_j\\exp(-\\|x_j\\|^{2})=a_j$, where $a_i$ and $a_j \\in\\mathbb{R}$ as $c_i$ and $c_j \\in\\mathbb{R}$.\n",
    "\n",
    "$\\exp(2x_i^{\\top}x_j) = 1+ \\frac{2x_i^{\\top}x_j}{1!}+\\frac{(2x_i^{\\top}x_j)^2}{2!}+\\frac{(2x_i^{\\top}x_j)^3}{3!}+\\dots$.\n",
    "\n",
    "1 - positive define kernel $ \\sum_{i=1}^{n}\\sum_{j=1}^{n}c_ic_j = (\\sum_{i=1}^{n}c_i)^2 \\geq 0$.\n",
    "\n",
    "$xy^{\\top}, \\: (xy^{\\top})^2, \\: (xy^{\\top})^3 \\dots$ - postitive define kernels, because they can be represented as dot product of function features. For example, $xy^{\\top} = f(x)f(y), \\: f(x) = [x_1, x_2, \\dots, x_n]^{\\top}$, $(xy^{\\top})^2 = f(x)f(y), \\: f(x) = [x_1^2, \\sqrt{2}x_1x_2, x_2^2]^{\\top}$, if $x = [x_1 x_2]^{\\top}$. \n",
    "\n",
    "Then $\\sum_{i=1}^{n}\\sum_{j=1}^{n}c_ic_jK_{ij} = \\sum_{i=1}^{n}\\sum_{j=1}^{n}a_ia_j\\exp(2x_i^{\\top}x_j)= \\sum_{i=1}^{n}\\sum_{j=1}^{n}a_ia_j(1+ \\frac{2x_i^{\\top}x_j}{1!}+\\frac{(2x_i^{\\top}x_j)^2}{2!}+\\frac{(2x_i^{\\top}x_j)^3}{3!}+\\dots) \\geq 0$, and $K(x,x')=\\exp(-\\|x-x'\\|^{2})$ positive define kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10*. Support Vector Machine (1 point)\n",
    "Show that for two-class SVM classifier the following upper bound on accuracy leave-one-out-cross-validation estimate holds true:\n",
    "$$L_{1}OCV=\\frac{1}{n}\\sum_{i=1}^{n}[y_{i} \\ne f_{i}(x_{i})]\\leq \\frac{|SV|}{n},$$\n",
    "where for all $i=1,\\dots,n$ $f_{i}(x_{i})$ is SVM fitted on the entire data without observation $(x_{i},y_{i})$ and $|SV|$ is the number of support vectors of SVM fit on the entire data.\n",
    "### Your solution:\n",
    "\n",
    "Support vectors are input vectors that just touch the boundary of the margin. Objects that lie outside the margin would be classified correctly regardless of whether they are part of the training set. But, support vectors define the linear separator and thus, if we removed one of them from the training set, it may be misclassified as a result.\n",
    "\n",
    "Consequently, the upper bound for leave-one-out-crossvalidation: $L_{1}OCV=\\frac{1}{n}\\sum_{i=1}^{n}[y_{i} \\ne f_{i}(x_{i})]\\leq \\frac{|SV|}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
